{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "#import pickle5 as pickle\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics  import f1_score,accuracy_score, confusion_matrix\n",
    "#from sklearn.ensemble import  RandomForestClassifier\n",
    "#import fasttext\n",
    "#import fasttext.util\n",
    "import jieba #tokenized = list(jieba.cut((line)))\n",
    "#ft = fasttext.load_model('../fastText/cc.zh.300.bin')\n",
    "#import sklearn_crfsuite\n",
    "\n",
    "#from sklearn_crfsuite import scorers\n",
    "import numpy as np\n",
    "#from sklearn_crfsuite import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "with open('./CRECIL/Final_Data/train.json','rb') as infile:\n",
    "    train_df = json.loads(infile.read())\n",
    "    \n",
    "\n",
    "with open('./CRECIL/Final_Data/train_1.pickle','rb') as infile:\n",
    "    train_df= pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/Final_Data/dev.json','rb') as infile:\n",
    "    dev_df = json.loads(infile.read())\n",
    "    \n",
    "# save version with extra field for tokenized/prepped dialogues\n",
    "with open('./CRECIL/Final_Data/dev_1.pickle','rb') as infile:\n",
    "    dev_df= pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/Final_Data/test.json','rb') as infile:\n",
    "    test_df = json.loads(infile.read())\n",
    "    \n",
    "with open('./CRECIL/Final_Data/test_1.pickle','rb') as infile:\n",
    "    test_df= pickle.load(infile)\n",
    "    \n",
    "#data load\n",
    "with open('./CRECIL/My_home_data/partition1.pickle','rb') as infile:\n",
    "    partition1 = pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/rid_to_rel.pickle','rb') as infile:\n",
    "    rid_to_rel = pickle.load(infile)\n",
    "\n",
    "with open('./CRECIL/rel_to_rid.pickle','rb') as infile:\n",
    "    rel_to_rid = pickle.load(infile)\n",
    "    \n",
    "    \n",
    "def get_blank_relations(annotations:list) -> list:\n",
    "    \"\"\"\n",
    "    Take the labels and clear out the gold-standard relations, \n",
    "    to be filled with predictions by model\n",
    "    \"\"\"\n",
    "    pred_list =[]\n",
    "    \n",
    "    for item in annotations:\n",
    "        copy = item.copy()\n",
    "        copy['r'] = []\n",
    "        copy['rid'] = []\n",
    "        pred_list.append(copy)\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def ch_tokenizer(input_str:str):\n",
    "    #tokenize sentence and return as list\n",
    "    tokenized = list(jieba.cut(input_str))\n",
    "    return tokenized\n",
    "\n",
    "def get_num_speakers(transcript:list)-> int:\n",
    "    \"\"\"\n",
    "    return number of speakers in scene\n",
    "    \"\"\"\n",
    "    ch_set = set()\n",
    "    for line in transcript:\n",
    "        ch_set.add(re.findall('S.*(?=:)',line)[0])\n",
    "    \n",
    "    total = len(ch_set)\n",
    "    return total\n",
    "\n",
    "def dummy_tokenize(phrase): #pass tokenized text version\n",
    "    return phrase\n",
    "\n",
    "class results:\n",
    "    \"\"\"\n",
    "    Make an object of the results so that I can get accurate numbers and confusion \n",
    "    matrices while sorting for labels.\n",
    "    \"\"\"\n",
    "    def __init__(self,y_test, y_pred):\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.labels = sorted(list(set(y_test)))\n",
    "        self.cm = pd.DataFrame(np.zeros(shape=(len(self.labels),len(self.labels)+1))) #initialize to empty\n",
    "        self.cm[0]=self.labels\n",
    "        self.cm.columns = ['pred']+self.labels\n",
    "        self.accuracy = 0\n",
    "        self.correct = 0\n",
    "        self.incorrect = 0\n",
    "        self.label_metrics = dict()\n",
    "        \n",
    "        for lab in self.labels:\n",
    "            self.label_metrics[lab] = dict()\n",
    "            self.label_metrics[lab]['F1'] = 0.0\n",
    "            self.label_metrics[lab]['Recall'] = 0.0\n",
    "            self.label_metrics[lab]['Accuracy'] = 0.0\n",
    "            self.label_metrics[lab]['TP'] = 0\n",
    "            self.label_metrics[lab]['TN'] = 0\n",
    "            self.label_metrics[lab]['FP'] = 0\n",
    "            self.label_metrics[lab]['FN'] = 0\n",
    "            self.label_metrics[lab]['cm'] = pd.DataFrame(np.zeros(shape=(2,3)),\\\n",
    "                                                         columns=['']+['Test P','Test N']) #initialize to empty\n",
    "            self.label_metrics[lab]['cm'][0] = ['Pred P','Pred N']\n",
    "        \n",
    "        for i in range(0,len(y_test)):\n",
    "            \n",
    "            #update CM\n",
    "            self.cm.loc[ self.cm['pred'] == y_pred[i], y_test[i]] +=1\n",
    "            \n",
    "            if y_pred[i]==y_test[i]:\n",
    "                self.correct +=1\n",
    "                self.label_metrics[y_test[i]]['TP'] +=1\n",
    "                for lab in self.labels:\n",
    "                    if lab != y_test[i]:\n",
    "                        self.label_metrics[lab]['TN'] +=1 #all other incorrect labels were not selected\n",
    "            else:\n",
    "                self.incorrect +=1\n",
    "                self.label_metrics[y_test[i]]['FN'] += 1 #true label was not selected\n",
    "                self.label_metrics[y_pred[i]]['FP'] +=1 #a wrong label was selected\n",
    "                \n",
    "                \n",
    "        self.accuracy = self.correct/(self.correct+self.incorrect)\n",
    "        for lab in self.labels:\n",
    "            \n",
    "            try:\n",
    "                self.label_metrics[lab]['Precision'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+self.label_metrics[lab]['FP'])\n",
    "                self.label_metrics[lab]['Recall'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+self.label_metrics[lab]['FN'])\n",
    "                self.label_metrics[lab]['F1'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+(0.5)*(self.label_metrics[lab]['FP']+self.label_metrics[lab]['FN']))      \n",
    "                \n",
    "            except:\n",
    "                self.label_metrics[lab]['Precision'] = 0\n",
    "                self.label_metrics[lab]['Recall'] = 0\n",
    "                self.label_metrics[lab]['F1'] = 0\n",
    "    def cm(self):\n",
    "        \"\"\"\n",
    "        show cm\n",
    "        \"\"\"\n",
    "        return(self.cm)\n",
    "    \n",
    "    def metric(self):\n",
    "        \n",
    "        \n",
    "        print(f\"Overall accuracy = {self.accuracy}\")\n",
    "        \n",
    "        for lab in self.labels:\n",
    "            print(f\"{lab}: {len([item for item in y_test if item==lab])} total\")\n",
    "            print(f\"   F1={self.label_metrics[lab]['F1']}\")\n",
    "            print(f\"   Precision={self.label_metrics[lab]['Precision']}\")\n",
    "            print(f\"   Recall={self.label_metrics[lab]['Recall']}\")\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "def jb_tokenize(line:str)-> list:\n",
    "    return list(jieba.cut((line)))\n",
    "\n",
    "\n",
    "def retrieve_s_lines(speaker:str,dialog:str)-> list:\n",
    "    # Retrieve lines spoken by speaker\n",
    "    lines =[]\n",
    "    \n",
    "    for line in dialog:\n",
    "        party = re.findall('^S\\s[0-9]',line)[0]\n",
    "        phrase = re.findall('(?<=: ).*',line)[0]\n",
    "        if party==speaker:\n",
    "            tokenized = ch_tokenizer(phrase)\n",
    "            lines.extend(tokenized)\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_mentions(ch:str,dialog:str)-> list:\n",
    "    lines =[]\n",
    "    \n",
    "    for line in dialog:\n",
    "        if bool(re.search(ch,line[1])):\n",
    "            lines.append(line[1])\n",
    "    return lines\n",
    "\n",
    "def get_avg_embeddings(array:list)-> dict:\n",
    "    \"\"\"Input array is list of sentences, output is a dictionary with the \n",
    "    average embeddings of those sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = []\n",
    "    for item in array:\n",
    "        embeddings.append(ft.get_sentence_vector(item))\n",
    "    avg_embeddings = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    return avg_embeddings\n",
    "\n",
    "def embeddings_to_features(array:list)-> dict:\n",
    "    \n",
    "    features = {}\n",
    "    for iv,value in enumerate(array):\n",
    "        features['v{}'.format(iv)]=value\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49af8670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 482/482 [00:09<00:00, 53.11it/s]\n"
     ]
    }
   ],
   "source": [
    "all_labels = list(rel_to_rid.keys())\n",
    "\n",
    "X_train = []\n",
    "y_train = dict()\n",
    "for lab in all_labels:\n",
    "    y_train[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(train_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = train_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(train_df[i][1])):\n",
    "        \n",
    "        rel_triplet = train_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        current_rels = []\n",
    "        for rel in list(set(r)):\n",
    "            if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "                x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "                y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "                combined = x_lines + y_lines\n",
    "                X_train.append(combined)\n",
    "\n",
    "                current_rels.append(rel)\n",
    "\n",
    "        if len(current_rels)>0:\n",
    "            for rel in current_rels:\n",
    "                y_train[rel].append(1)\n",
    "\n",
    "            counter_examples_rels = list(set(all_labels).difference(set(current_rels)))\n",
    "            for rel in counter_examples_rels:\n",
    "                y_train[rel].append(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2206c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:01<00:00, 61.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_dev = []\n",
    "y_dev = dict()\n",
    "for lab in all_labels:\n",
    "    y_dev[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(dev_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = dev_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(dev_df[i][1])):\n",
    "        \n",
    "        rel_triplet = dev_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        current_rels = []\n",
    "        for rel in list(set(r)):\n",
    "            if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "                x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "                y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "                combined = x_lines + y_lines\n",
    "                X_dev.append(combined)\n",
    "\n",
    "                current_rels.append(rel)\n",
    "\n",
    "        if len(current_rels)>0:\n",
    "            for rel in current_rels:\n",
    "                y_dev[rel].append(1)\n",
    "\n",
    "            counter_examples_rels = list(set(all_labels).difference(set(current_rels)))\n",
    "            for rel in counter_examples_rels:\n",
    "                y_dev[rel].append(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f67e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [00:01<00:00, 59.16it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = dict()\n",
    "for lab in all_labels:\n",
    "    y_test[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(test_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = test_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(test_df[i][1])):\n",
    "        \n",
    "        rel_triplet = test_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        current_rels = []\n",
    "        for rel in list(set(r)):\n",
    "            if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "                x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "                y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "                combined = x_lines + y_lines\n",
    "                X_test.append(combined)\n",
    "\n",
    "                current_rels.append(rel)\n",
    "                \n",
    "        if len(current_rels)>0:\n",
    "            for lab in all_labels:\n",
    "                if lab in current_rels:\n",
    "                    y_test[lab].append(1)\n",
    "                else:\n",
    "                    y_test[lab].append(0)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a25dac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f9fea2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['per:friends',\n",
       " 'per:dates',\n",
       " 'per:subordinate',\n",
       " 'per:teacher',\n",
       " 'per:parents-in-law',\n",
       " 'per:nurse',\n",
       " 'per:alternate_name',\n",
       " 'per:relative',\n",
       " 'per:grandchildren',\n",
       " 'per:classmate',\n",
       " 'per:negative impression',\n",
       " 'per:spouse',\n",
       " 'per:positive impression',\n",
       " 'per:client',\n",
       " 'per:acquaintance',\n",
       " 'per:parents',\n",
       " 'per:neighbor',\n",
       " 'per:boyfriend',\n",
       " 'per:colleague',\n",
       " 'per:children-in-law',\n",
       " 'per:children',\n",
       " 'per:siblings',\n",
       " 'per:boss',\n",
       " 'per:ex-girlfriend',\n",
       " 'per:roommate',\n",
       " 'per:nickname',\n",
       " 'unanswerable',\n",
       " 'per:ex-boyfriend',\n",
       " 'per:siblings-in-law',\n",
       " 'per:girlfriend',\n",
       " 'per:student',\n",
       " 'per:grandparents']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(all_labels).difference(set(current_rels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45e3f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature word list\n",
    "with open('full_word_list.pickle','rb') as infile:\n",
    "    word_list = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fae27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_word_counts(text,word_list):\n",
    "    \n",
    "    \n",
    "    output_list = []\n",
    "    for word in word_list:\n",
    "        output_list.append(len([item for item in text if item==word]))\n",
    "        \n",
    "    output = pd.Series(output_list)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a650d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_train])\n",
    "X_train_features.columns = word_list\n",
    "\n",
    "X_dev_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_dev])\n",
    "X_dev_features.columns = word_list\n",
    "\n",
    "X_test_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_test])\n",
    "X_test_features.columns = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ac64e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "GNB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b106bc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7468"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "526c5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7134"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c70ca96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'per:roommate'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bf98cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                    | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7468, 35450]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449454/135146539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mGNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_pred_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[1;32m    262\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7468, 35450]"
     ]
    }
   ],
   "source": [
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "\n",
    "for lab in tqdm(all_labels):\n",
    "    GNB.fit(X_train_features, y_train[lab])\n",
    "    y_pred_dev = GNB.predict(X_dev_features)\n",
    "    y_pred_test = GNB.predict(X_test_features)\n",
    "    \n",
    "    y_dev_true.append(y_dev[lab])\n",
    "    y_dev_pred.append(y_pred_dev)\n",
    "    \n",
    "    y_test_true.append(y_test[lab])\n",
    "    y_test_pred.append(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "710ea558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  8.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('ref_files/DT_score_df.pickle','wb') as outfile:\\n    pickle.dump(score_df,outfile)\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "\"\"\"with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\"\"\"\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "NB= DecisionTreeClassifier()\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    DT.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = DT.predict(X_dev_features)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c3162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_limited)\n",
    "X_train_df.columns = ['text']\n",
    "X_train_features = X_train_df.text.apply(lambda x:make_word_counts(x,word_list))\n",
    "X_train_features.columns = word_list\n",
    "\n",
    "\n",
    "X_dev_df = pd.DataFrame(X_dev_limited)\n",
    "X_dev_df.columns = ['text']\n",
    "X_dev_features = X_dev_df.text.apply(lambda x:make_word_counts(x,word_list))\n",
    "X_dev_features.columns = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b9b02e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42c927a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_dev_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed4ee102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT= DecisionTreeClassifier()\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    DT.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = DT.predict(X_dev_features)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acd14229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.458247</td>\n",
       "      <td>0.471606</td>\n",
       "      <td>0.464831</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.375685</td>\n",
       "      <td>0.300159</td>\n",
       "      <td>0.333702</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.04955</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.205275</td>\n",
       "      <td>0.139191</td>\n",
       "      <td>0.165894</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.272189</td>\n",
       "      <td>0.23273</td>\n",
       "      <td>0.248222</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.458247  0.471606  0.464831      3293\n",
       "1        family   0.375685  0.300159  0.333702      2512\n",
       "2  occupational    0.04955  0.019964  0.028461       551\n",
       "3    friendship   0.205275  0.139191  0.165894      1286\n",
       "4           Avg   0.272189   0.23273  0.248222    1910.5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4095df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "MNB.fit(X_train_features, y_train_limited)\n",
    "y_pred = MNB.predict(X_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf32b726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075581395348837"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev_limited,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "434ac907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/elyeb/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv = CountVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3),min_df=2,max_df=0.55)\n",
    "\n",
    "#train_array,\n",
    "X_train_cv = cv.fit_transform(X_train_limited)\n",
    "\n",
    "X_train_array = X_train_cv.toarray()\n",
    "\n",
    "X_dev_cv = cv.transform(X_dev_limited)\n",
    "X_dev_array = X_dev_cv.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa574f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "\n",
    "MNB.fit(X_train_array, y_train_limited)\n",
    "y_pred = MNB.predict(X_dev_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c2b1b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev_limited,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc11187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BNB = BernoulliNB()\n",
    "\n",
    "BNB.fit(X_train_features, y_train_limited)\n",
    "y_pred = BNB.predict(X_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d675ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 1168, 'friendship': 144, 'other': 402, 'occupational': 6})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86c8f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 854, 'friendship': 202, 'other': 593, 'occupational': 71})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_dev_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc39d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 2507,\n",
       "         'friendship': 1223,\n",
       "         'other': 3234,\n",
       "         'occupational': 458})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in list(set(y_train_m)):\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48a420c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 28.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.273307</td>\n",
       "      <td>0.367272</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.413514</td>\n",
       "      <td>0.121815</td>\n",
       "      <td>0.188192</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.266003</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.345961</td>\n",
       "      <td>0.151014</td>\n",
       "      <td>0.207943</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.559701  0.273307  0.367272      3293\n",
       "1        family   0.413514  0.121815  0.188192      2512\n",
       "2  occupational   0.144628  0.063521  0.088272       551\n",
       "3    friendship   0.266003  0.145412  0.188034      1286\n",
       "4           Avg   0.345961  0.151014  0.207943    1910.5"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "y_pred_probs_all = []\n",
    "\n",
    "for grp in tqdm(all_groups):\n",
    "    MNB.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = MNB.predict(X_dev_features)\n",
    "    y_pred_probs = MNB.predict_proba(X_dev_features)\n",
    "    prob_1 = [item[1] for item in y_pred_probs]\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    y_pred_probs_all.append(prob_1)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "\"\"\"with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\"\"\"\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c867d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.448578</td>\n",
       "      <td>0.699362</td>\n",
       "      <td>0.546576</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.344169</td>\n",
       "      <td>0.960987</td>\n",
       "      <td>0.506823</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.071009</td>\n",
       "      <td>0.833031</td>\n",
       "      <td>0.130862</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.173751</td>\n",
       "      <td>0.940902</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.259377</td>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.369399</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.448578  0.699362  0.546576      3293\n",
       "1        family   0.344169  0.960987  0.506823      2512\n",
       "2  occupational   0.071009  0.833031  0.130862       551\n",
       "3    friendship   0.173751  0.940902  0.293333      1286\n",
       "4           Avg   0.259377  0.858571  0.369399    1910.5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337f7559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e904eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35450, 183)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47d1b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7422, 183)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3),min_df=2,max_df=0.55)\n",
    "\n",
    "#train_array,\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "X_train_array = X_train_cv.toarray()\n",
    "\n",
    "X_dev_cv = cv.transform(X_dev)\n",
    "X_dev_array = X_dev_cv.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ba8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e10efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3)) #,min_df=2,max_df=0.55\n",
    "X_train_hash = hv.fit_transform(X_train)\n",
    "X_dev_hash = hv.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71fb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35450, 1048576)\n",
      "(7422, 1048576)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_hash.shape)\n",
    "print(X_dev_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2af74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ref_files/X_train_hash.pickle','wb') as outfile:\n",
    "    pickle.dump(X_train_hash,outfile)\n",
    "\n",
    "with open('ref_files/X_dev_hash.pickle','wb') as outfile:\n",
    "    pickle.dump(X_dev_hash,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b40898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_array.pickle','wb') as outfile:\n",
    "    pickle.dump(X_train_array,outfile)\n",
    "\n",
    "with open('ref_files/X_dev_array.pickle','wb') as outfile:\n",
    "    pickle.dump(X_dev_array,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_array.pickle','rb') as infile:\n",
    "    X_train_array =pickle.load(infile)\n",
    "\n",
    "with open('ref_files/X_dev_array.pickle','rb') as infile:\n",
    "    X_dev_array = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a93413",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs_all = []\n",
    "\n",
    "y_pred_probs = GBC.predict_proba(X_dev_array)\n",
    "    prob_1 = [item[1] for item in y_pred_probs]\n",
    "    y_pred_probs_all.append(prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_hash.pickle','rb') as infile:\n",
    "    X_train_hash =pickle.load(infile)\n",
    "\n",
    "with open('ref_files/X_dev_hash.pickle','rb') as infile:\n",
    "    X_dev_hash = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d4f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = MNB.predict_proba(X_dev_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc2d980a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_451183/4180030825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_probs_all_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_probs_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "y_pred_probs_all.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc1db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs_all_df = pd.DataFrame(y_pred_probs_all).transpose()\n",
    "y_pred_probs_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/y_pred_probs_all_df.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_probs_all_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "156d1e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.231732815625558e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc83ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a00a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/dev_gt.pickle','wb') as outfile:\n",
    "    pickle.dump(y_test_all_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/MNB_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/MNB_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#fit\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    KNN.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = GBC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/KNN_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/KNN_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfde0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#fit\n",
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    GBC.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = GBC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/GBC_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/GBC_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#fit\n",
    "SGD = SGDClassifier()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    SGD.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = SGD.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/SGD_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/SGD_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#fit\n",
    "SVM = SVC()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    SVM.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = SVM.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/SVM_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/SVM_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc017694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR= LogisticRegression()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    LR.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = LR.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/LR_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/LR_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#fit\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    RFC.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = RFC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/RFC_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/RFC_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "#fit\n",
    "NB = GaussianNB()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    NB.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = NB.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/GaussianNB_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/GaussianNB_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
