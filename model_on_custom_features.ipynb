{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14e2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics  import f1_score,accuracy_score, confusion_matrix\n",
    "import jieba \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "with open('./CRECIL/Final_Data/train.json','rb') as infile:\n",
    "    train_df = json.loads(infile.read())\n",
    "    \n",
    "\n",
    "with open('./CRECIL/Final_Data/train_1.pickle','rb') as infile:\n",
    "    train_df= pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/Final_Data/dev.json','rb') as infile:\n",
    "    dev_df = json.loads(infile.read())\n",
    "    \n",
    "# save version with extra field for tokenized/prepped dialogues\n",
    "with open('./CRECIL/Final_Data/dev_1.pickle','rb') as infile:\n",
    "    dev_df= pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/Final_Data/test.json','rb') as infile:\n",
    "    test_df = json.loads(infile.read())\n",
    "    \n",
    "with open('./CRECIL/Final_Data/test_1.pickle','rb') as infile:\n",
    "    test_df= pickle.load(infile)\n",
    "    \n",
    "#data load\n",
    "with open('./CRECIL/My_home_data/partition1.pickle','rb') as infile:\n",
    "    partition1 = pickle.load(infile)\n",
    "    \n",
    "with open('./CRECIL/rid_to_rel.pickle','rb') as infile:\n",
    "    rid_to_rel = pickle.load(infile)\n",
    "\n",
    "with open('./CRECIL/rel_to_rid.pickle','rb') as infile:\n",
    "    rel_to_rid = pickle.load(infile)\n",
    "    \n",
    "    \n",
    "def get_blank_relations(annotations:list) -> list:\n",
    "    \"\"\"\n",
    "    Take the labels and clear out the gold-standard relations, \n",
    "    to be filled with predictions by model\n",
    "    \"\"\"\n",
    "    pred_list =[]\n",
    "    \n",
    "    for item in annotations:\n",
    "        copy = item.copy()\n",
    "        copy['r'] = []\n",
    "        copy['rid'] = []\n",
    "        pred_list.append(copy)\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def ch_tokenizer(input_str:str):\n",
    "    #tokenize sentence and return as list\n",
    "    tokenized = list(jieba.cut(input_str))\n",
    "    return tokenized\n",
    "\n",
    "def get_num_speakers(transcript:list)-> int:\n",
    "    \"\"\"\n",
    "    return number of speakers in scene\n",
    "    \"\"\"\n",
    "    ch_set = set()\n",
    "    for line in transcript:\n",
    "        ch_set.add(re.findall('S.*(?=:)',line)[0])\n",
    "    \n",
    "    total = len(ch_set)\n",
    "    return total\n",
    "\n",
    "def dummy_tokenize(phrase): #pass tokenized text version\n",
    "    return phrase\n",
    "\n",
    "class results:\n",
    "    \"\"\"\n",
    "    Make an object of the results so that I can get accurate numbers and confusion \n",
    "    matrices while sorting for labels.\n",
    "    \"\"\"\n",
    "    def __init__(self,y_test, y_pred):\n",
    "        self.y_test = y_test\n",
    "        self.y_pred = y_pred\n",
    "        self.labels = sorted(list(set(y_test)))\n",
    "        self.cm = pd.DataFrame(np.zeros(shape=(len(self.labels),len(self.labels)+1))) #initialize to empty\n",
    "        self.cm[0]=self.labels\n",
    "        self.cm.columns = ['pred']+self.labels\n",
    "        self.accuracy = 0\n",
    "        self.correct = 0\n",
    "        self.incorrect = 0\n",
    "        self.label_metrics = dict()\n",
    "        \n",
    "        for lab in self.labels:\n",
    "            self.label_metrics[lab] = dict()\n",
    "            self.label_metrics[lab]['F1'] = 0.0\n",
    "            self.label_metrics[lab]['Recall'] = 0.0\n",
    "            self.label_metrics[lab]['Accuracy'] = 0.0\n",
    "            self.label_metrics[lab]['TP'] = 0\n",
    "            self.label_metrics[lab]['TN'] = 0\n",
    "            self.label_metrics[lab]['FP'] = 0\n",
    "            self.label_metrics[lab]['FN'] = 0\n",
    "            self.label_metrics[lab]['cm'] = pd.DataFrame(np.zeros(shape=(2,3)),\\\n",
    "                                                         columns=['']+['Test P','Test N']) #initialize to empty\n",
    "            self.label_metrics[lab]['cm'][0] = ['Pred P','Pred N']\n",
    "        \n",
    "        for i in range(0,len(y_test)):\n",
    "            \n",
    "            #update CM\n",
    "            self.cm.loc[ self.cm['pred'] == y_pred[i], y_test[i]] +=1\n",
    "            \n",
    "            if y_pred[i]==y_test[i]:\n",
    "                self.correct +=1\n",
    "                self.label_metrics[y_test[i]]['TP'] +=1\n",
    "                for lab in self.labels:\n",
    "                    if lab != y_test[i]:\n",
    "                        self.label_metrics[lab]['TN'] +=1 #all other incorrect labels were not selected\n",
    "            else:\n",
    "                self.incorrect +=1\n",
    "                self.label_metrics[y_test[i]]['FN'] += 1 #true label was not selected\n",
    "                self.label_metrics[y_pred[i]]['FP'] +=1 #a wrong label was selected\n",
    "                \n",
    "                \n",
    "        self.accuracy = self.correct/(self.correct+self.incorrect)\n",
    "        for lab in self.labels:\n",
    "            \n",
    "            try:\n",
    "                self.label_metrics[lab]['Precision'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+self.label_metrics[lab]['FP'])\n",
    "                self.label_metrics[lab]['Recall'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+self.label_metrics[lab]['FN'])\n",
    "                self.label_metrics[lab]['F1'] = self.label_metrics[lab]['TP']/(self.label_metrics[lab]['TP']+(0.5)*(self.label_metrics[lab]['FP']+self.label_metrics[lab]['FN']))      \n",
    "                \n",
    "            except:\n",
    "                self.label_metrics[lab]['Precision'] = 0\n",
    "                self.label_metrics[lab]['Recall'] = 0\n",
    "                self.label_metrics[lab]['F1'] = 0\n",
    "    def cm(self):\n",
    "        \"\"\"\n",
    "        show cm\n",
    "        \"\"\"\n",
    "        return(self.cm)\n",
    "    \n",
    "    def metric(self):\n",
    "        \n",
    "        \n",
    "        print(f\"Overall accuracy = {self.accuracy}\")\n",
    "        \n",
    "        for lab in self.labels:\n",
    "            print(f\"{lab}: {len([item for item in y_test if item==lab])} total\")\n",
    "            print(f\"   F1={self.label_metrics[lab]['F1']}\")\n",
    "            print(f\"   Precision={self.label_metrics[lab]['Precision']}\")\n",
    "            print(f\"   Recall={self.label_metrics[lab]['Recall']}\")\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "def jb_tokenize(line:str)-> list:\n",
    "    return list(jieba.cut((line)))\n",
    "\n",
    "\n",
    "def retrieve_s_lines(speaker:str,dialog:str)-> list:\n",
    "    # Retrieve lines spoken by speaker\n",
    "    lines =[]\n",
    "    \n",
    "    for line in dialog:\n",
    "        party = re.findall('^S\\s[0-9]',line)[0]\n",
    "        phrase = re.findall('(?<=: ).*',line)[0]\n",
    "        if party==speaker:\n",
    "            tokenized = ch_tokenizer(phrase)\n",
    "            lines.extend(tokenized)\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_mentions(ch:str,dialog:str)-> list:\n",
    "    lines =[]\n",
    "    \n",
    "    for line in dialog:\n",
    "        if bool(re.search(ch,line[1])):\n",
    "            lines.append(line[1])\n",
    "    return lines\n",
    "\n",
    "def get_avg_embeddings(array:list)-> dict:\n",
    "    \"\"\"Input array is list of sentences, output is a dictionary with the \n",
    "    average embeddings of those sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = []\n",
    "    for item in array:\n",
    "        embeddings.append(ft.get_sentence_vector(item))\n",
    "    avg_embeddings = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    return avg_embeddings\n",
    "\n",
    "def embeddings_to_features(array:list)-> dict:\n",
    "    \n",
    "    features = {}\n",
    "    for iv,value in enumerate(array):\n",
    "        features['v{}'.format(iv)]=value\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34995655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/482 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.465 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|███████████████████████████████████████| 482/482 [00:08<00:00, 53.63it/s]\n"
     ]
    }
   ],
   "source": [
    "all_labels = list(rel_to_rid.keys())\n",
    "\n",
    "X_train = []\n",
    "y_train = dict()\n",
    "for lab in all_labels:\n",
    "    y_train[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(train_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = train_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(train_df[i][1])):\n",
    "        \n",
    "        rel_triplet = train_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "            x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "            y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "            combined = x_lines + y_lines\n",
    "            X_train.append(combined)\n",
    "            current_rels = []\n",
    "            \n",
    "            for rel in list(set(r)):\n",
    "\n",
    "                current_rels.append(rel)\n",
    "                \n",
    "            if len(current_rels)>0:\n",
    "                for lab in all_labels:\n",
    "                    if lab in current_rels:\n",
    "                        y_train[lab].append(1)\n",
    "                    else:\n",
    "                        y_train[lab].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe45e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 116/116 [00:01<00:00, 65.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_dev = []\n",
    "y_dev = dict()\n",
    "for lab in all_labels:\n",
    "    y_dev[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(dev_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = dev_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(dev_df[i][1])):\n",
    "        \n",
    "        rel_triplet = dev_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "            x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "            y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "            combined = x_lines + y_lines\n",
    "            X_dev.append(combined)\n",
    "            current_rels = []\n",
    "            \n",
    "            for rel in list(set(r)):\n",
    "                current_rels.append(rel)\n",
    "                \n",
    "            if len(current_rels)>0:\n",
    "                for lab in all_labels:\n",
    "                    if lab in current_rels:\n",
    "                        y_dev[lab].append(1)\n",
    "                    else:\n",
    "                        y_dev[lab].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bb5b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 71/71 [00:01<00:00, 63.17it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = dict()\n",
    "for lab in all_labels:\n",
    "    y_test[lab]= []\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(test_df))): \n",
    "    \n",
    "    #list of tuples containing speakers and tokenized sentences\n",
    "    dialog = test_df[i][0]\n",
    "    \n",
    "    #get relations and labels:\n",
    "    for j in range(0,len(test_df[i][1])):\n",
    "        \n",
    "        rel_triplet = test_df[i][1][j]\n",
    "        x = rel_triplet['x']\n",
    "        y = rel_triplet['y']\n",
    "        r = rel_triplet['r'] #note that this is a list that can have len>1\n",
    "\n",
    "        # retrieve lines by speakers or entity mentionned:\n",
    "        if bool(re.search('^S\\s[0-9]',x)) and bool(re.search('^S\\s[0-9]',y)):\n",
    "            x_lines = retrieve_s_lines(x,dialog)\n",
    "\n",
    "            y_lines = retrieve_s_lines(y,dialog)\n",
    "\n",
    "            combined = x_lines + y_lines\n",
    "            X_test.append(combined)\n",
    "            current_rels = []\n",
    "            \n",
    "            for rel in list(set(r)):\n",
    "\n",
    "\n",
    "\n",
    "                    current_rels.append(rel)\n",
    "                \n",
    "            if len(current_rels)>0:\n",
    "                for lab in all_labels:\n",
    "                    if lab in current_rels:\n",
    "                        y_test[lab].append(1)\n",
    "                    else:\n",
    "                        y_test[lab].append(0)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e3f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature word list\n",
    "with open('full_word_list.pickle','rb') as infile:\n",
    "    word_list = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c09c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chinese_mw.txt','r') as infile:\n",
    "    mws = infile.read().split('\\n')\n",
    "mws =mws[0:len(mws)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c82db24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = word_list+ mws\n",
    "word_list = list(set(word_list))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature list for readers\n",
    "with open('custom_feature_set.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fae27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_word_counts(text,word_list):\n",
    "    \n",
    "    \n",
    "    output_list = []\n",
    "    for word in word_list:\n",
    "        output_list.append(len([item for item in text if item==word]))\n",
    "        \n",
    "    output = pd.Series(output_list)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a650d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_train])\n",
    "X_train_features.columns = word_list\n",
    "\n",
    "X_dev_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_dev])\n",
    "X_dev_features.columns = word_list\n",
    "\n",
    "X_test_features = pd.DataFrame([make_word_counts(x,word_list) for x in X_test])\n",
    "X_test_features.columns = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "523a2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "GNB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072ce0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 32/32 [00:01<00:00, 30.19it/s]\n"
     ]
    }
   ],
   "source": [
    "y_dev_true = []\n",
    "y_dev_pred = []\n",
    "\n",
    "y_test_true = []\n",
    "y_test_pred = []\n",
    "\n",
    "for lab in tqdm(all_labels):\n",
    "    GNB.fit(X_train_features, y_train[lab])\n",
    "    y_pred_dev = GNB.predict(X_dev_features)\n",
    "    y_pred_test = GNB.predict(X_test_features)\n",
    "    \n",
    "    y_dev_true.append(y_dev[lab])\n",
    "    y_dev_pred.append(y_pred_dev)\n",
    "    \n",
    "    y_test_true.append(y_test[lab])\n",
    "    y_test_pred.append(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_true_df = pd.DataFrame(y_dev_true)\n",
    "y_dev_true_df = y_dev_true_df.transpose()\n",
    "y_dev_true_df.columns = all_labels\n",
    "\n",
    "y_dev_pred_df = pd.DataFrame(y_dev_pred)\n",
    "y_dev_pred_df = y_dev_pred_df.transpose()\n",
    "y_dev_pred_df.columns = all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1e7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_labels:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_dev_true_df[item], y_dev_pred_df[item], labels=all_labels,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_dev_true_df[item], y_dev_pred_df[item], labels=all_labels,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_dev_true_df[item], y_dev_pred_df[item], labels=all_labels,average='binary'))\n",
    "    true_labs.append(Counter(y_dev_true_df[item])[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "710ea558",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precision','recall','f1','support']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2689114",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev_score_df.csv','w') as outfile:\n",
    "    score_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585165fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test_true_df = pd.DataFrame(y_test_true)\n",
    "y_test_true_df = y_test_true_df.transpose()\n",
    "y_test_true_df.columns = all_labels\n",
    "\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred)\n",
    "y_test_pred_df = y_test_pred_df.transpose()\n",
    "y_test_pred_df.columns = all_labels\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_labels:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_true_df[item], y_test_pred_df[item], labels=all_labels,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_true_df[item], y_test_pred_df[item], labels=all_labels,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_true_df[item], y_test_pred_df[item], labels=all_labels,average='binary'))\n",
    "    true_labs.append(Counter(y_test_true_df[item])[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc8d1b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>per:children</td>\n",
       "      <td>0.094474</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.171382</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>per:spouse</td>\n",
       "      <td>0.055344</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.103757</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>per:parents</td>\n",
       "      <td>0.094474</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.171382</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unanswerable</td>\n",
       "      <td>0.388073</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.551499</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>per:alternate_name</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>per:parents-in-law</td>\n",
       "      <td>0.036517</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.070334</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>per:grandparents</td>\n",
       "      <td>0.02834</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>per:children-in-law</td>\n",
       "      <td>0.036517</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.070334</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>per:grandchildren</td>\n",
       "      <td>0.02834</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.054956</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>per:nurse</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.086</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>per:friends</td>\n",
       "      <td>0.027184</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>per:classmate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>per:neighbor</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.109767</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>per:acquaintance</td>\n",
       "      <td>0.031026</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.059429</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>per:negative impression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>per:ex-girlfriend</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>per:girlfriend</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>per:dates</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>per:ex-boyfriend</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>per:boyfriend</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>per:relative</td>\n",
       "      <td>0.049251</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.09073</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>per:siblings</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>per:positive impression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>per:colleague</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>per:client</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>per:siblings-in-law</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>per:subordinate</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>per:boss</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>per:student</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>per:teacher</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>per:nickname</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>per:roommate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.450431</td>\n",
       "      <td>0.055753</td>\n",
       "      <td>39.09375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     labels precision    recall        f1   support\n",
       "0              per:children  0.094474  0.921739  0.171382       115\n",
       "1                per:spouse  0.055344  0.828571  0.103757        70\n",
       "2               per:parents  0.094474  0.921739  0.171382       115\n",
       "3              unanswerable  0.388073  0.952703  0.551499       444\n",
       "4        per:alternate_name       0.0       0.0       0.0         0\n",
       "5        per:parents-in-law  0.036517   0.95122  0.070334        41\n",
       "6          per:grandparents   0.02834  0.903226  0.054956        31\n",
       "7       per:children-in-law  0.036517   0.95122  0.070334        41\n",
       "8         per:grandchildren   0.02834  0.903226  0.054956        31\n",
       "9                 per:nurse  0.045073  0.934783     0.086        46\n",
       "10              per:friends  0.027184  0.823529  0.052632        34\n",
       "11            per:classmate       0.0       0.0       0.0         6\n",
       "12             per:neighbor  0.058185  0.967213  0.109767        61\n",
       "13         per:acquaintance  0.031026  0.702703  0.059429        37\n",
       "14  per:negative impression       0.0       0.0       0.0         1\n",
       "15        per:ex-girlfriend       0.0       0.0       0.0         1\n",
       "16           per:girlfriend       0.0       0.0       0.0         2\n",
       "17                per:dates  0.004854       0.5  0.009615         4\n",
       "18         per:ex-boyfriend       0.0       0.0       0.0         1\n",
       "19            per:boyfriend       0.0       0.0       0.0         2\n",
       "20             per:relative  0.049251     0.575   0.09073        80\n",
       "21             per:siblings  0.040541  0.576923  0.075758        52\n",
       "22  per:positive impression       0.0       0.0       0.0         2\n",
       "23            per:colleague  0.007353       0.5  0.014493         8\n",
       "24               per:client  0.001976       0.5  0.003937         2\n",
       "25      per:siblings-in-law       0.0       0.0       0.0         0\n",
       "26          per:subordinate  0.008427       0.5  0.016575        12\n",
       "27                 per:boss  0.008427       0.5  0.016575        12\n",
       "28              per:student       0.0       0.0       0.0         0\n",
       "29              per:teacher       0.0       0.0       0.0         0\n",
       "30             per:nickname       0.0       0.0       0.0         0\n",
       "31             per:roommate       0.0       0.0       0.0         0\n",
       "32                      Avg  0.032637  0.450431  0.055753  39.09375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precision','recall','f1','support']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb246d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_score_df.csv','w') as outfile:\n",
    "    score_df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "NB= DecisionTreeClassifier()\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    DT.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = DT.predict(X_dev_features)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c3162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_limited)\n",
    "X_train_df.columns = ['text']\n",
    "X_train_features = X_train_df.text.apply(lambda x:make_word_counts(x,word_list))\n",
    "X_train_features.columns = word_list\n",
    "\n",
    "\n",
    "X_dev_df = pd.DataFrame(X_dev_limited)\n",
    "X_dev_df.columns = ['text']\n",
    "X_dev_features = X_dev_df.text.apply(lambda x:make_word_counts(x,word_list))\n",
    "X_dev_features.columns = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b9b02e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1720, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42c927a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_dev_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed4ee102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT= DecisionTreeClassifier()\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    DT.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = DT.predict(X_dev_features)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acd14229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.458247</td>\n",
       "      <td>0.471606</td>\n",
       "      <td>0.464831</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.375685</td>\n",
       "      <td>0.300159</td>\n",
       "      <td>0.333702</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.04955</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.205275</td>\n",
       "      <td>0.139191</td>\n",
       "      <td>0.165894</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.272189</td>\n",
       "      <td>0.23273</td>\n",
       "      <td>0.248222</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.458247  0.471606  0.464831      3293\n",
       "1        family   0.375685  0.300159  0.333702      2512\n",
       "2  occupational    0.04955  0.019964  0.028461       551\n",
       "3    friendship   0.205275  0.139191  0.165894      1286\n",
       "4           Avg   0.272189   0.23273  0.248222    1910.5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4095df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "MNB.fit(X_train_features, y_train_limited)\n",
    "y_pred = MNB.predict(X_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf32b726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075581395348837"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev_limited,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "434ac907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/elyeb/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv = CountVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3),min_df=2,max_df=0.55)\n",
    "\n",
    "#train_array,\n",
    "X_train_cv = cv.fit_transform(X_train_limited)\n",
    "\n",
    "X_train_array = X_train_cv.toarray()\n",
    "\n",
    "X_dev_cv = cv.transform(X_dev_limited)\n",
    "X_dev_array = X_dev_cv.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa574f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "\n",
    "MNB.fit(X_train_array, y_train_limited)\n",
    "y_pred = MNB.predict(X_dev_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c2b1b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_dev_limited,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc11187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BNB = BernoulliNB()\n",
    "\n",
    "BNB.fit(X_train_features, y_train_limited)\n",
    "y_pred = BNB.predict(X_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d675ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 1168, 'friendship': 144, 'other': 402, 'occupational': 6})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86c8f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 854, 'friendship': 202, 'other': 593, 'occupational': 71})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_dev_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc39d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 2507,\n",
       "         'friendship': 1223,\n",
       "         'other': 3234,\n",
       "         'occupational': 458})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in list(set(y_train_m)):\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48a420c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00, 28.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.273307</td>\n",
       "      <td>0.367272</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.413514</td>\n",
       "      <td>0.121815</td>\n",
       "      <td>0.188192</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.063521</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.266003</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.345961</td>\n",
       "      <td>0.151014</td>\n",
       "      <td>0.207943</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.559701  0.273307  0.367272      3293\n",
       "1        family   0.413514  0.121815  0.188192      2512\n",
       "2  occupational   0.144628  0.063521  0.088272       551\n",
       "3    friendship   0.266003  0.145412  0.188034      1286\n",
       "4           Avg   0.345961  0.151014  0.207943    1910.5"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "y_pred_probs_all = []\n",
    "\n",
    "for grp in tqdm(all_groups):\n",
    "    MNB.fit(X_train_features, y_train[grp]['y'])\n",
    "    y_pred = MNB.predict(X_dev_features)\n",
    "    y_pred_probs = MNB.predict_proba(X_dev_features)\n",
    "    prob_1 = [item[1] for item in y_pred_probs]\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    y_pred_probs_all.append(prob_1)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "\"\"\"with open('ref_files/DT_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\"\"\"\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "\"\"\"\n",
    "with open('ref_files/DT_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)\n",
    "\"\"\"\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c867d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>precisions</th>\n",
       "      <th>recalls</th>\n",
       "      <th>f1s</th>\n",
       "      <th>true_labs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>0.448578</td>\n",
       "      <td>0.699362</td>\n",
       "      <td>0.546576</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "      <td>0.344169</td>\n",
       "      <td>0.960987</td>\n",
       "      <td>0.506823</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occupational</td>\n",
       "      <td>0.071009</td>\n",
       "      <td>0.833031</td>\n",
       "      <td>0.130862</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friendship</td>\n",
       "      <td>0.173751</td>\n",
       "      <td>0.940902</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.259377</td>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.369399</td>\n",
       "      <td>1910.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels precisions   recalls       f1s true_labs\n",
       "0         other   0.448578  0.699362  0.546576      3293\n",
       "1        family   0.344169  0.960987  0.506823      2512\n",
       "2  occupational   0.071009  0.833031  0.130862       551\n",
       "3    friendship   0.173751  0.940902  0.293333      1286\n",
       "4           Avg   0.259377  0.858571  0.369399    1910.5"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337f7559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e904eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35450, 183)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47d1b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7422, 183)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3),min_df=2,max_df=0.55)\n",
    "\n",
    "#train_array,\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "X_train_array = X_train_cv.toarray()\n",
    "\n",
    "X_dev_cv = cv.transform(X_dev)\n",
    "X_dev_array = X_dev_cv.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ba8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e10efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = HashingVectorizer(tokenizer=ch_tokenizer,ngram_range=(1,3)) #,min_df=2,max_df=0.55\n",
    "X_train_hash = hv.fit_transform(X_train)\n",
    "X_dev_hash = hv.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71fb495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35450, 1048576)\n",
      "(7422, 1048576)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_hash.shape)\n",
    "print(X_dev_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2af74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ref_files/X_train_hash.pickle','wb') as outfile:\n",
    "    pickle.dump(X_train_hash,outfile)\n",
    "\n",
    "with open('ref_files/X_dev_hash.pickle','wb') as outfile:\n",
    "    pickle.dump(X_dev_hash,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b40898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_array.pickle','wb') as outfile:\n",
    "    pickle.dump(X_train_array,outfile)\n",
    "\n",
    "with open('ref_files/X_dev_array.pickle','wb') as outfile:\n",
    "    pickle.dump(X_dev_array,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_array.pickle','rb') as infile:\n",
    "    X_train_array =pickle.load(infile)\n",
    "\n",
    "with open('ref_files/X_dev_array.pickle','rb') as infile:\n",
    "    X_dev_array = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a93413",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs_all = []\n",
    "\n",
    "y_pred_probs = GBC.predict_proba(X_dev_array)\n",
    "    prob_1 = [item[1] for item in y_pred_probs]\n",
    "    y_pred_probs_all.append(prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays seem to be large and cause other scripts to fail. Try saving them for easier loading:\n",
    "with open('ref_files/X_train_hash.pickle','rb') as infile:\n",
    "    X_train_hash =pickle.load(infile)\n",
    "\n",
    "with open('ref_files/X_dev_hash.pickle','rb') as infile:\n",
    "    X_dev_hash = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d4f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = MNB.predict_proba(X_dev_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc2d980a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_451183/4180030825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_probs_all_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_probs_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "y_pred_probs_all.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc1db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs_all_df = pd.DataFrame(y_pred_probs_all).transpose()\n",
    "y_pred_probs_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/y_pred_probs_all_df.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_probs_all_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "156d1e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.231732815625558e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(prob_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc83ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a00a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/dev_gt.pickle','wb') as outfile:\n",
    "    pickle.dump(y_test_all_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/MNB_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/MNB_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#fit\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    KNN.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = GBC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/KNN_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/KNN_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfde0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#fit\n",
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    GBC.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = GBC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/GBC_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/GBC_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914165c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#fit\n",
    "SGD = SGDClassifier()\n",
    "\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    SGD.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = SGD.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/SGD_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/SGD_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#fit\n",
    "SVM = SVC()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    SVM.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = SVM.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/SVM_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/SVM_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc017694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR= LogisticRegression()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    LR.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = LR.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/LR_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/LR_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#fit\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    RFC.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = RFC.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/RFC_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/RFC_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "#fit\n",
    "NB = GaussianNB()\n",
    "\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for grp in all_groups:\n",
    "    NB.fit(X_train_array, y_train[grp]['y'])\n",
    "    y_pred = NB.predict(X_dev_array)\n",
    "    \n",
    "    y_test_all.append(y_dev[grp]['y'])\n",
    "    y_pred_all.append(y_pred)\n",
    "    \n",
    "y_test_all_df = pd.DataFrame(y_test_all)\n",
    "y_test_all_df = y_test_all_df.transpose()\n",
    "y_test_all_df.columns = all_groups\n",
    "\n",
    "y_pred_all_df = pd.DataFrame(y_pred_all)\n",
    "y_pred_all_df = y_pred_all_df.transpose()\n",
    "y_pred_all_df.columns = all_groups\n",
    "\n",
    "with open('ref_files/GaussianNB_pred.pickle','wb') as outfile:\n",
    "    pickle.dump(y_pred_all_df,outfile)\n",
    "    \n",
    "labels = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "true_labs = []\n",
    "f1s = []\n",
    "\n",
    "for item in all_groups:\n",
    "    labels.append(item)\n",
    "    precisions.append(sklearn.metrics.precision_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    recalls.append(sklearn.metrics.recall_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_test_all_df[item], y_pred_all_df[item], labels=all_groups,average='binary'))\n",
    "    true_labs.append(Counter(y_test_all_df[item])[1])\n",
    "    \n",
    "labels.append('Avg')\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "f1s.append(np.mean(f1s))\n",
    "true_labs.append(np.mean(true_labs))\n",
    "\n",
    "score_df = pd.DataFrame([labels,precisions,recalls,f1s,true_labs])\n",
    "score_df = score_df.transpose()\n",
    "score_df.columns = ['labels','precisions','recalls','f1s','true_labs']\n",
    "\n",
    "with open('ref_files/GaussianNB_score_df.pickle','wb') as outfile:\n",
    "    pickle.dump(score_df,outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
